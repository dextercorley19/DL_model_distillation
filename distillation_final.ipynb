{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48e72b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcmac14/Library/Caches/pypoetry/virtualenvs/deeplearning-4xETOx6U-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", \n",
    "    pretrained=\"laion2b_s34b_b79k\"\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "labels = [\n",
    "    \"an airplane\",\n",
    "    \"a bird\",\n",
    "    \"a car\",\n",
    "    \"a cat\",\n",
    "    \"a deer\",\n",
    "    \"a dog\",\n",
    "    \"a horse\",\n",
    "    \"a monkey\",\n",
    "    \"a ship\",\n",
    "    \"a truck\"\n",
    "]\n",
    "\n",
    "text = tokenizer(labels)\n",
    "text_embeddings = model.encode_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d038dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9dcbd5",
   "metadata": {},
   "source": [
    "Now, embeddings contains a 512 length vector for each text prompt. This vector has the same size as the vision encoder output. The dot product of the vector with vision features indicates the similarity, so we can determine the class probabilities for our dataset as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3901fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def embeddings_to_class_probs(vision_embeddings, text_embeddings):\n",
    "    vision_embeddings = vision_embeddings / vision_embeddings.norm(dim=-1, keepdim=True)\n",
    "    text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "    logits = vision_embeddings @ text_embeddings.T\n",
    "    class_probs = F.softmax(100. * logits, dim=-1)\n",
    "    return class_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6d34b",
   "metadata": {},
   "source": [
    "Now that we have the text embeddings for our target task, and a method to compare these against the image embeddings, all that's left to do is run the STL10 dataset through OpenCLIP vision encoder, compute the output class probabilities, and compare the result against the ground truth label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f456d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [04:51<00:00, 27.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torchvision.datasets import STL10\n",
    "\n",
    "dataset = STL10(\n",
    "    root=\"./stl10\",\n",
    "    download=True,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "num_correct = 0\n",
    "\n",
    "for image, label in tqdm.tqdm(dataset):\n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    vision_embeddings = model.encode_image(input_tensor)\n",
    "    output_class_probs = embeddings_to_class_probs(vision_embeddings, text_embeddings)\n",
    "    output_label = torch.argmax(output_class_probs, dim=-1)\n",
    "    num_correct += int(torch.count_nonzero(output_label == label))\n",
    "\n",
    "accuracy = 100. * num_correct / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32088797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.675"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e9b96d",
   "metadata": {},
   "source": [
    "And after this, out of the box, the OpenCLIP encoder, without any additional training, get's 96.68% accuracy on the STL10 test dataset! With no tricks, we achieved fairly competitive accuracy on the STL10 dataset, for comparison you can see other competitive results on the STL10 dataset here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bbd84",
   "metadata": {},
   "source": [
    "Using linear head for classification\n",
    "\n",
    "As shown, using the text prompts as class labels, we were able to achieve pretty good accuracy on the STL10 dataset without any training or ground truth labels. But what if we have ground truth labels available? Can we use this to improve the accuracy?\n",
    "\n",
    "With this option, we'll explore how we can use some ground truth data to train a tiny logistic regression layer (linear layer followed by softmax) at the end of the OpenCLIP model and see if this improves the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec7f35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.7450\n",
      "Epoch 2/10, Loss: 0.8890\n",
      "Epoch 3/10, Loss: 0.4993\n",
      "Epoch 4/10, Loss: 0.3268\n",
      "Epoch 5/10, Loss: 0.2386\n",
      "Epoch 6/10, Loss: 0.1871\n",
      "Epoch 7/10, Loss: 0.1549\n",
      "Epoch 8/10, Loss: 0.1337\n",
      "Epoch 9/10, Loss: 0.1159\n",
      "Epoch 10/10, Loss: 0.1016\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "linear_probe = nn.Linear(512, len(labels))\n",
    "linear_probe.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(linear_probe.parameters(), lr=3e-4)\n",
    "\n",
    "train_dataset = STL10(\n",
    "    root=\"./stl10\",\n",
    "    download=True,\n",
    "    split=\"train\",\n",
    "    transform=preprocess\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    linear_probe.train()\n",
    "    total_loss = 0\n",
    "    for image, label in iter(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Run open-clip to get vision embeddings\n",
    "        with torch.no_grad():\n",
    "            vision_embeddings = model.encode_image(image)\n",
    "            if vision_embeddings.dtype == torch.float16:\n",
    "                vision_embeddings = vision_embeddings.float()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output_logits = linear_probe(vision_embeddings)\n",
    "\n",
    "        loss = F.cross_entropy(output_logits, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62ed06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test set: 100%|██████████| 125/125 [01:31<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the linear probe on the test set\n",
    "test_dataset = STL10(\n",
    "    root=\"./stl10\",\n",
    "    download=True,\n",
    "    split=\"test\",\n",
    "    transform=preprocess\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "linear_probe.eval() \n",
    "model.eval() \n",
    "\n",
    "num_correct_test = 0\n",
    "total_test_samples = 0\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for image, label in tqdm.tqdm(test_loader, desc=\"Evaluating on test set\"):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Run open-clip to get vision embeddings\n",
    "        vision_embeddings = model.encode_image(image)\n",
    "        if vision_embeddings.dtype == torch.float16:\n",
    "            vision_embeddings = vision_embeddings.float()\n",
    "        \n",
    "        output_logits = linear_probe(vision_embeddings)\n",
    "        \n",
    "        # Get predicted labels\n",
    "        _, predicted_labels = torch.max(output_logits, 1)\n",
    "        \n",
    "        num_correct_test += (predicted_labels == label).sum().item()\n",
    "        total_test_samples += label.size(0)\n",
    "\n",
    "test_accuracy = 100. * num_correct_test / total_test_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7932aa9",
   "metadata": {},
   "source": [
    "After training the linear probe, we evaluate it on the STL10 dataset, similar to before, and our accuracy is now 98.57!\n",
    "\n",
    "Great! By using some labeled data, we were able to train a small logistic regression layer that improves the accuracy of OpenCLIP on the STL10 dataset by nearly +2%!\n",
    "\n",
    "This improvement is likely because our text prompts, like \"an airplane\", might not perfectly match the labels as they appear in the STL10 dataset. But by seeing a few examples for each label, we can learn reference embeddings that more accurately represent the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1c592",
   "metadata": {},
   "source": [
    "## Training a student model to mimic OpenCLIP\n",
    "\n",
    "We've now seen that using the large OpenCLIP model, we can achieve competetive results on the STL10 image classification dataset with little effort. But OpenCLIP is large, and is likely to have high memory consumption and latency compared to other model architectures. In addition, as a vision transformer model, OpenCLIP is less capable of exploiting the Deep Learning Accelerator (DLA) on Jetson AGX Orin, given the matrix multiplication in the attention layers. CNN models, like resnet18, on the other hand are highly optimized by both the GPU and DLA on Jetson, and allow us to run models at higher throughput and with less memory.\n",
    "\n",
    "However, knowledge distillation can impact the accuracy of the model, so we'd like to better understand what factors are most important. To do this, we've run a few experiments that seek to answer a few questions:\n",
    "\n",
    "How does distillation compare to training with ground truth labels?\n",
    "How does the data distribution used for training impact model accuracy?\n",
    "How does the distillation method impact model accuracy? Is it better to train on the class probabilities or internal features?\n",
    "How does the student model architecture impact model accuracy? Will resnet50 obtain higher accuracy than resnet18?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5529da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcmac14/Library/Caches/pypoetry/virtualenvs/deeplearning-4xETOx6U-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "5000it [00:00, 8787.94it/s]\n",
      "8000it [00:00, 9257.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps for train_model_from_scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcmac14/Library/Caches/pypoetry/virtualenvs/deeplearning-4xETOx6U-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Test Loss: 1.7259, Test Accuracy: 34.29%\n",
      "Epoch 2 - Test Loss: 1.6783, Test Accuracy: 39.52%\n",
      "Epoch 3 - Test Loss: 1.4484, Test Accuracy: 46.79%\n",
      "Epoch 4 - Test Loss: 1.3064, Test Accuracy: 51.29%\n",
      "Epoch 5 - Test Loss: 1.2840, Test Accuracy: 52.86%\n",
      "Epoch 6 - Test Loss: 1.2543, Test Accuracy: 53.89%\n",
      "Epoch 7 - Test Loss: 1.3261, Test Accuracy: 55.14%\n",
      "Epoch 8 - Test Loss: 1.2820, Test Accuracy: 57.17%\n",
      "Epoch 9 - Test Loss: 1.3840, Test Accuracy: 56.45%\n",
      "Epoch 10 - Test Loss: 1.3339, Test Accuracy: 57.70%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from stl10_utils import (\n",
    "    precompute_clip_stl10_train_image_embeddings,\n",
    "    precompute_clip_stl10_test_image_embeddings,\n",
    "    precompute_clip_stl10_text_embeddings,\n",
    "    train_resnet18_from_scratch,\n",
    "    train_resnet18_linear_probe_train_only\n",
    ")\n",
    "\n",
    "precompute_clip_stl10_train_image_embeddings()\n",
    "precompute_clip_stl10_test_image_embeddings()\n",
    "precompute_clip_stl10_text_embeddings()\n",
    "train_resnet18_from_scratch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad1ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps for probe model in train_student_linear_probe\n",
      "Probe checkpoint data/experiments/train_probe_model_linear/checkpoint_9.pth not found. Attempting to train probe model...\n",
      "Using device: mps for train_probe_model_linear\n",
      "Starting training for linear probe model. Output will be in data/experiments/train_probe_model_linear\n",
      "Using device: mps for train_probe_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:12<00:00,  6.20it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 0 | TRAIN LOSS 3.7658857170836875 | TEST ACC 97.138 |\n",
      "Saving checkpoint for epoch 0 to data/experiments/train_probe_model_linear/checkpoint_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 79/79 [00:11<00:00,  6.69it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 1 | TRAIN LOSS 0.2560871586723612 | TEST ACC 97.388 |\n",
      "Saving checkpoint for epoch 1 to data/experiments/train_probe_model_linear/checkpoint_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 79/79 [00:11<00:00,  6.71it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 2 | TRAIN LOSS 0.1538377963898914 | TEST ACC 97.412 |\n",
      "Saving checkpoint for epoch 2 to data/experiments/train_probe_model_linear/checkpoint_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 79/79 [00:11<00:00,  6.65it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 3 | TRAIN LOSS 0.07637745670479079 | TEST ACC 97.537 |\n",
      "Saving checkpoint for epoch 3 to data/experiments/train_probe_model_linear/checkpoint_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 79/79 [00:11<00:00,  6.72it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 4 | TRAIN LOSS 0.048721451878915974 | TEST ACC 97.688 |\n",
      "Saving checkpoint for epoch 4 to data/experiments/train_probe_model_linear/checkpoint_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 79/79 [00:11<00:00,  6.69it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 5 | TRAIN LOSS 0.02843519240430204 | TEST ACC 97.4 |\n",
      "Saving checkpoint for epoch 5 to data/experiments/train_probe_model_linear/checkpoint_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 79/79 [00:11<00:00,  6.70it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 6 | TRAIN LOSS 0.02611771381016021 | TEST ACC 97.5 |\n",
      "Saving checkpoint for epoch 6 to data/experiments/train_probe_model_linear/checkpoint_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 79/79 [00:11<00:00,  6.72it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 7 | TRAIN LOSS 0.026856589847502027 | TEST ACC 97.6 |\n",
      "Saving checkpoint for epoch 7 to data/experiments/train_probe_model_linear/checkpoint_7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 79/79 [00:11<00:00,  6.73it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 8 | TRAIN LOSS 0.014373054670230478 | TEST ACC 97.638 |\n",
      "Saving checkpoint for epoch 8 to data/experiments/train_probe_model_linear/checkpoint_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 79/79 [00:11<00:00,  6.64it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EPOCH 9 | TRAIN LOSS 0.008312782411593696 | TEST ACC 97.812 |\n",
      "Saving checkpoint for epoch 9 to data/experiments/train_probe_model_linear/checkpoint_9.pth\n",
      "Finished training for linear probe model.\n",
      "Loading probe model weights from data/experiments/train_probe_model_linear/checkpoint_9.pth\n",
      "Starting student model training (arch: resnet18) using linear probe.\n",
      "Using device: mps for train_student_classification_model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_resnet18_linear_probe_train_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/USF/spring2/deeplearning/stl10_utils.py:405\u001b[0m, in \u001b[0;36mtrain_resnet18_linear_probe_train_only\u001b[0;34m()\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_resnet18_linear_probe_train_only\u001b[39m():\n\u001b[0;32m--> 405\u001b[0m     \u001b[43mtrain_student_linear_probe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/experiments/train_resnet18_linear_probe_train_only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43march\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet18\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_stl10_train_embedding_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/USF/spring2/deeplearning/stl10_utils.py:306\u001b[0m, in \u001b[0;36mtrain_student_linear_probe\u001b[0;34m(output_dir, arch, temperature, train_dataset, test_dataset)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting student model training (arch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00march\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) using linear probe.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# train_student_classification_model is from openclip_utils\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m \u001b[43mtrain_student_classification_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43march\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSTL10_LABELS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Adjusted for broader compatibility\u001b[39;49;00m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobe_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobe_model_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished student model training using linear probe.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/USF/spring2/deeplearning/openclip_utils.py:327\u001b[0m, in \u001b[0;36mtrain_student_classification_model\u001b[0;34m(output_dir, model, train_dataset, test_dataset, learning_rate, batch_size, num_workers, num_epochs, text_embeddings, probe_model, temperature, seed)\u001b[0m\n\u001b[1;32m    318\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m    320\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    321\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m    322\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    323\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    324\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers\n\u001b[1;32m    325\u001b[0m )\n\u001b[0;32m--> 327\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (probe_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (text_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deeplearning-4xETOx6U-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:385\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 385\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deeplearning-4xETOx6U-py3.10/lib/python3.10/site-packages/torch/utils/data/sampler.py:155\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/deeplearning-4xETOx6U-py3.10/lib/python3.10/site-packages/torch/utils/data/sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "train_resnet18_linear_probe_train_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ecdacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stl10 import (\n",
    "    precompute_clip_stl10_train_image_embeddings,\n",
    "    precompute_clip_stl10_unlabeled_image_embeddings,\n",
    "    precompute_clip_stl10_test_image_embeddings,\n",
    "    precompute_clip_stl10_text_embeddings,\n",
    "    train_resnet18_linear_probe\n",
    ")\n",
    "\n",
    "precompute_clip_stl10_train_image_embeddings()\n",
    "precompute_clip_stl10_unlabeled_image_embeddings()\n",
    "precompute_clip_stl10_test_image_embeddings()\n",
    "precompute_clip_stl10_text_embeddings()\n",
    "train_resnet18_linear_probe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-4xETOx6U-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
